{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "Practica1_Reconocimiento_Formas.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2-AngM7IWxnf",
        "MHLaQDLZW6xr",
        "dnNZ-i-WXEEw",
        "nZGQSU-GwuSs",
        "7cc4NAogplN0",
        "X3KaWBnrfXgp",
        "J6k6vchsfcRi",
        "v15yk3XCfgfG"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LaertesPecker/Clasificadores/blob/master/Practica1_Reconocimiento_Formas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx7EE02AplNp",
        "colab_type": "text"
      },
      "source": [
        "# PRÁCTICA 1: Clasificadores Euclídeo y Bayesiano\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiRCjRQSplNt",
        "colab_type": "text"
      },
      "source": [
        "## 1. Introducción\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig1kQTgeBNaJ",
        "colab_type": "text"
      },
      "source": [
        "A lo largo de esta práctica veremos como construir diversos clasificadores. También veremos cuándo y por qué son útiles así como medir sus rendimientos. Finalmente, aplicaremos los conocimientos adquiridos en un problema real y veremos la potencia que tienen estos sencillos clasificadores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-AngM7IWxnf",
        "colab_type": "text"
      },
      "source": [
        "## 2. Clasificador de la distancia euclídea"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHLaQDLZW6xr",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 En qué consiste el clasificador y como lo hemos implementado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8WfsMIjplNw",
        "colab_type": "text"
      },
      "source": [
        "El clasificador de la distancia euclídea parte de la hipótesis de que dispersion de las clases es pequeña en relación a la distancia entre ellas. Una vez aceptada la hipótesis, el representante de cada clase será el centroide de todos los datos pertenecientes a esa clase. Por tanto, el primer paso para impementar en clasificador será calcular los centroides de los datos de test. Una vez calculados los centroides, para determinar la pertenencia de un dato a una clase, calcularemos la distancia del dato a todos los centroides y el dato pertenecerá a la clase que representa el centroide más cercano a él. \n",
        "\n",
        "Matemáticamente, el cálculo del centroide es: $z_i = \\frac{1}{card(\\alpha_i)}\\sum_{x \\in \\alpha_i} x$, donde $\\alpha_i$ representa a la clase i. Como ya se ha expuesto arriba, la función discriminante será la distancia euclídea $f_i(x)=d_E(x,z_i)$. Por tanto, se clasificará un objeto en la clase cuya distancia\n",
        "sea la mínima, es decir $x \\in \\alpha_i \\leftrightarrow arg min_i{f_i(x)}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnNZ-i-WXEEw",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Implementación del clasificador"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPEH8KHRW2bm",
        "colab_type": "text"
      },
      "source": [
        "Para implementar el clasificador se deberán definir 4 funciones: La función de **fit**, la cual calculará los centroides de nuestros datos de test. La función **predict**, la cual calculará la distancia de todos nuestros datos los cuales queremos obtener su clase a todos los centroides calculados en la función fit. La función **pred_label**, que cogerá la etiqueta del centroide mas cercano a todos los datos que queremos predecir. Finalmente, la función **num_aciertos**, que calculará el número de aciertos de nuestro clasificador.\n",
        "\n",
        "Empezamos importando los paquetes necesarios y creando la clase y el constructor de la misma:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ezy3rBUmwr6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from abc import abstractmethod\n",
        "\n",
        "class Classifier:\n",
        "    @abstractmethod\n",
        "    def fit(self,X,y):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self,X):\n",
        "        pass\n",
        "\n",
        "class ClassifEuclid(Classifier):\n",
        "    def __init__(self,labels=[]):\n",
        "        self.labels=labels\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQa1Iu7U7BM_",
        "colab_type": "text"
      },
      "source": [
        "A continuación, definimos la función fit:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w99NNecF7E6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(self,X,y):\n",
        "        self = np.array([np.mean(X[y==l],axis=0) for l in np.unique(y)])\n",
        "        return self"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKjKaH8f7RBe",
        "colab_type": "text"
      },
      "source": [
        "Como se ha descrito arriba, la función fit calcula los centroides, para ello recorre todas las x con misma etiqueta y calcula su media. A continuación se procede a escribir la función predict:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrbJDVVj7eZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(self,X):\n",
        "        distancias = np.linalg.norm(centroides[:,np.newaxis] - X,axis=2)\n",
        "        return distancias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFELkfNJ88aH",
        "colab_type": "text"
      },
      "source": [
        "Calcula la norma entre los centroides y los valores de x, es decir, calcula la distancia euclídea y la almacena en la variable distancias. Hay que añadiendo una nueva columna o hacer un reshape de centroides para poder operar con x, haciendo el reshape para los datos del iris deberíamos cambiar la matriz de centroides con:\n",
        "\n",
        "```\n",
        "centroides.reshape(3,1,4)\n",
        "```\n",
        "Continuamos definiendo la función pred_label:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IXQiusm9VBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pred_label(self,X):\n",
        "        distancias = predict(self,X)\n",
        "        pred = np.argmin(distancias,axis=0)\n",
        "        return pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7kDy0_N9sGj",
        "colab_type": "text"
      },
      "source": [
        "Finalmente, calculamos el número de aciertos y el porcentaje de aciertos de nuestro clasificador:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzGncrl99zdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def num_aciertos(self,X,y):\n",
        "        pred = pred_label(self,X)\n",
        "        numAciertos = (pred == y).sum()\n",
        "        porcAciertos = (pred==y).mean()*100\n",
        "        print(f\"Numero de aciertos: {numAciertos} , Porcetaje de Aciertos: {porcAciertos}\" )\n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfz_4SniVA9Z",
        "colab_type": "text"
      },
      "source": [
        "Definimos una función más que utilizaremos cuando queramos utilizar una parte de los datos para entrenar nuestro clasificador, por defecto entrenaremos con el 70% de los datos y testearemos con el otro 30%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjp8wVLGUewB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clasfEucl(X,y,name,test_size=0.3):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
        "  centroides = np.array([np.mean(X_train[y_train==l],axis=0) for l in np.unique(y_train)])\n",
        "  distancias = np.linalg.norm(centroides[:,np.newaxis] - X_test,axis=2)\n",
        "  pred = np.argmin(distancias,axis=0)\n",
        "  numAciertos = (pred == y_test).sum()\n",
        "  porcAciertos = (pred == y_test).mean()*100\n",
        "  print(f\"Datos \" + name + \":\")\n",
        "  print(f\"Numero de aciertos: {numAciertos} , Porcetaje de Aciertos: {porcAciertos}\" )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brs0dSFk7sSq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.covariance import ShrunkCovariance\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn import preprocessing\n",
        "from abc import abstractmethod\n",
        "\n",
        "\n",
        "class Classifier(BaseEstimator):\n",
        "\n",
        "    @abstractmethod\n",
        "    def fit(self, X, y):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self, X):\n",
        "        pass\n",
        "\n",
        "\n",
        "class ClassifEuclid(Classifier):\n",
        "    def __init__(self):\n",
        "        \"\"\"Constructor de la clase\n",
        "        labels: lista de etiquetas de esta clase\"\"\"\n",
        "        self.centroids = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Entrena el clasificador\n",
        "        X: matriz numpy cada fila es un dato, cada columna una medida\n",
        "        y: vector de etiquetas, tantos elementos como filas en X\n",
        "        retorna objeto clasificador\"\"\"\n",
        "        assert X.ndim == 2 and X.shape[0] == len(y)\n",
        "        \n",
        "        # Entrena el clasificador\n",
        "        self.centroids = np.array([np.mean(X[y==l],axis=0) for l in np.unique(y)])\n",
        "        return self\n",
        "        \n",
        "        return self\n",
        "\n",
        "    def decision_function(self, X):\n",
        "        \"\"\"Estima el grado de pertenencia de cada dato a todas las clases\n",
        "        X: matriz numpy cada fila es un dato, cada columna una medida del vector de caracteristicas.\n",
        "        Retorna una matriz, con tantas filas como datos y tantas columnas como clases tenga\n",
        "        el problema, cada fila almacena los valores pertenencia de un dato a cada clase\"\"\"\n",
        "        assert self.centroids is not None, \"Error: The classifier needs to be fitted. Please call fit(X, y) method.\"\n",
        "        assert X.ndim == 2 and X.shape[1] == self.centroids.shape[1]\n",
        "\n",
        "        # Calcula y devuelve la distancia a cada centroide\n",
        "        distancias = np.linalg.norm(self.centroids[:,np.newaxis] - X,axis=2)\n",
        "        return distancias\n",
        "    def predict(self, X):\n",
        "        \"\"\"Estima la etiqueta de cada dato. La etiqueta puede ser un entero o bien un string.\n",
        "        X: matriz numpy cada fila es un dato, cada columna una medida\n",
        "        retorna un vector con las etiquetas de cada dato\"\"\"\n",
        "        # Calcula y devuelve la clase más probable\n",
        "        distancias = self.decision_function(X)\n",
        "        pred = np.argmin(distancias,axis=0)\n",
        "        return pred\n",
        "    def score (self,X,y):\n",
        "        self.fit(X,y)\n",
        "        pred = self.predict(X)\n",
        "        porcAciertos = (pred==y).mean()*100\n",
        "        return porcAciertos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZGQSU-GwuSs",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 Tabla de resultados. Discusión"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taST0BHcIHDc",
        "colab_type": "text"
      },
      "source": [
        "#### 2.3.1 Tabla de resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC1hr5LtXO2B",
        "colab_type": "text"
      },
      "source": [
        "Utilizamos nuestro clasificador para los datos de iris, wine y cancer. Para entrenar nuestro clasificador utilizaremos el 70% de los datos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6LeQF_pS-hw",
        "colab_type": "code",
        "outputId": "5e196423-ba10-49a0-863e-364b99c899f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from sklearn.datasets import  load_iris, load_wine, load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "X, y  = load_iris(return_X_y=True)\n",
        "clasfEucl(X,y,\"Iris\",0.5)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Datos Iris:\n",
            "Numero de aciertos: 72 , Porcetaje de Aciertos: 96.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axisYgAYTM7X",
        "colab_type": "code",
        "outputId": "9d2df4e2-bd15-4922-9360-930542a9ce55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "X, y  = load_wine(return_X_y=True)\n",
        "clasfEucl(X,y,\"Wine\",0.5)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Datos Wine:\n",
            "Numero de aciertos: 63 , Porcetaje de Aciertos: 70.78651685393258\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbzFmiCBR5Vy",
        "colab_type": "code",
        "outputId": "8b1124bf-b32b-4028-fe43-cee9f2c8d0e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "X, y =load_breast_cancer(return_X_y=True)\n",
        "clasfEucl(X,y,\"Cancer\",0.5)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Datos Cancer:\n",
            "Numero de aciertos: 243 , Porcetaje de Aciertos: 85.26315789473684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRLoX27lGMsf",
        "colab_type": "text"
      },
      "source": [
        "Medida |Iris | Wine | Cancer\n",
        "--- |--- | --- | ---\n",
        "Numero Aciertos|42 | 42 | 155\n",
        "Porcentaje Aciertos|92% | 70% | 90%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qndlNLFNvPn",
        "colab_type": "text"
      },
      "source": [
        "#### 2.3.2 Discusión\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "at0EMTNfGK57",
        "colab_type": "text"
      },
      "source": [
        "Como se puede observar a partir de la tabla, las bases de datos de iris y cancer obtienen muy buenos resultados de alrededor del 90% de aciertos. No obstante wine obtiene un porcentaje de aciertos del 70% aproximadamente. Esto puede ser debido a que los datos wine tienen una dispersión de las clases mayor en relación con la distancia entre ellas que la que pudiera haber en los datos del iris o del cancer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cc4NAogplN0",
        "colab_type": "text"
      },
      "source": [
        "## 3. Clasificador Estadístico Bayesiano\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3KaWBnrfXgp",
        "colab_type": "text"
      },
      "source": [
        "### 3.1 En qué consiste el clasificador y como lo hemos implementado\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wktwuFuHgSIq",
        "colab_type": "text"
      },
      "source": [
        "La medida de pertenencia ahora va a estar establecida por la estadística bayesiana. Cuando usabamos un clasificador euclídeo, la dispersión debía ser pequeña dentro de una clase para que funcionara bien. En este tema vamos a resolver el problema de clasificación mediante el modelado estadístico de la distribución de las muestras en casa clase. Este clasificador solo funcionará bien para clases que se distribuyan unimodalmente. Otro inconveniente de este clasificador es que el número de parámetros crece cuadráticamente. No obstante, este clasificador es muy simple matemáticamente y se adecua a muchos problemas. A menudo, es imposible encontrar una frontera que separe perfectamente dos clases. Vamos a minimizar este error con la estadística.\n",
        "\n",
        "Para nuestro clasificador usaremos la probabilidad a posteriori pero, gracias al teorema de Bayes, conociendo las demás podemos conocer la a posteriori: \n",
        "\n",
        "$P(\\alpha_i|x) = \\frac{p(x|\\alpha_i)P(\\alpha_i)}{p(x)}$\n",
        "\n",
        "Una forma de clasificar, por ejemplo, seria asignarle la que tenga mayor numerador. Por tanto, la funcion discriminante la podremos escribir como:\n",
        "    \n",
        "$f_j(x) = P(\\alpha_j|x)$\n",
        "\n",
        "$f_j(x) = p(\\alpha_j,x) = p(x|\\alpha_j)P(\\alpha_j)$ (conjunta)\n",
        "\n",
        "Ambas son compatibles porque solo difieren en una constante, p(x).\n",
        "\n",
        "Vamos a suponer que la condicionada en cada clase sigue una gausiana normal de media μ. Entrenar nuestro clasificador sera encontrar la media y la variancia para cada clase. En una gausiana, el exponente es parecido a una distancia (si sigma vale uno o es la identidad es una distancia). Una vez calculemos la media, varianza, probabilidad a priori y la inversa de la varianza, sustituiremos los valores en la función discriminante:\n",
        "\n",
        "$d_i(x) = -\\frac{1}{2}ln|\\Sigma_i| - \\frac{1}{2}(x-\\mu_i)^T\\Sigma_i^{-1}(x-\\mu_i) + ln P(\\alpha_i)$\n",
        "\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6k6vchsfcRi",
        "colab_type": "text"
      },
      "source": [
        "### 3.2 Implementación del clasificador"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM-I0ChMnlwt",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Nuestra función **fit** se encargará de calcular la probabilidad a priori (pues es independiente de X), la matriz de covarianzas y la media de cada clase. Quedando:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP1GK_tHjuj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_estadistico(X,y):\n",
        "  unique, counts = np.unique(y,return_counts=True)\n",
        "  self.labels = unique\n",
        "  prob_priori = np.divide(counts,y.sum())\n",
        "  ln_priori = np.log(prob_priori)\n",
        "  self.ln_apriories = ln_priori\n",
        "  self.means = np.array([np.mean(X[y==l],axis=0) for l in np.unique(y)])\n",
        "  cov = np.array([np.cov(X[y==l] - self.means[l], rowvar=False) for l in np.unique(y)])\n",
        "  det = np.array(np.linalg.det(cov))\n",
        "  self.ln_determinants = np.log(det)\n",
        "  self.inv_covs = np.linalg.inv(cov)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjowipPjkAhX",
        "colab_type": "text"
      },
      "source": [
        "Nuestra función predict, calculará la resta entre las X y las medias y computará la función discriminante:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQqviBM1kIij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_estadistico(X,y):\n",
        "  mean_x0 = np.array(X - slef.means[:,np.newaxis])\n",
        "  result = np.empty(((len(self.labels),np.shape(X)[0])))\n",
        "  for j in range(0,np.shape(mean_x0)[0]):\n",
        "    result[j] = -0.5*self.ln_determinants[j] + -0.5*(mean_x0[j].dot(self.inv_covs[j])*mean_x0[j]).sum(1) + slef.ln_apriories[j]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JppJQ9z9ldzn",
        "colab_type": "text"
      },
      "source": [
        "La funcion predict simplemente se quedará con los argumentos máximos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eUUtkcglh5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_label_estadistico(self,X):\n",
        "  pred = np.argmax(result,axis=0)\n",
        "  num_aciertos = (pred == y).sum()\n",
        "  media_aciertos = (pred == y).mean()*100\n",
        "  print(f\"Numero de Aciertos es: {num_aciertos}, Porcentaje de aciertos: {media_aciertos}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fqhJi74lbJx",
        "colab_type": "text"
      },
      "source": [
        "Quedando la clase completa como:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybEYikMJkVCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from abc import abstractmethod\n",
        "\n",
        "class Classifier:\n",
        "\n",
        "    @abstractmethod\n",
        "    def fit(self,X,y):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self,X):\n",
        "        pass\n",
        "\n",
        "class ClassifBayesiano(Classifier):\n",
        "    def __init__(self):\n",
        "        \"\"\"Constructor de la clase\n",
        "        labels: lista de etiquetas de esta clase\"\"\"\n",
        "        self.ln_apriories = None\n",
        "        self.means = None\n",
        "        self.ln_determinants = None\n",
        "        self.inv_covs = None\n",
        "    def fit_estadistico(self,X,y):\n",
        "        assert X.ndim == 2 and X.shape[0] == len(y)\n",
        "        unique, counts = np.unique(y,return_counts=True)\n",
        "        self.labels = unique\n",
        "        prob_priori = np.divide(counts,y.sum())\n",
        "        ln_priori = np.log(prob_priori)\n",
        "        self.ln_apriories = ln_priori\n",
        "        self.means = np.array([np.mean(X[y==l],axis=0) for l in np.unique(y)])\n",
        "        cov = np.array([np.cov(X[y==l] - self.means[l], rowvar=False) for l in np.unique(y)])\n",
        "        det = np.array(np.linalg.det(cov))\n",
        "        self.ln_determinants = np.log(det)\n",
        "        self.inv_covs = np.linalg.inv(cov)\n",
        "        return self\n",
        "    def predict_estadistico(self,X):\n",
        "        assert self.means is not None, \"Error: The classifier needs to be fitted. Please call fit(X, y) method.\"\n",
        "        assert X.ndim == 2 and X.shape[1] == self.means.shape[1]\n",
        "        mean_x0 = np.array(X - self.means[:,np.newaxis])\n",
        "        # Nuestra variable resultado tendrá tamaño nºclases x nº datos\n",
        "        result = np.empty(((len(self.labels),np.shape(X)[0])))\n",
        "        # Hacemos los calculos para cada clase, multiplicamos mean_x0i por invi y luego hacemos el producto elemento por elemento con * y lo sumamos con sum(1). Esto es porque mean_x0[clase] tiene tamaños nºdatos x nºclases,\n",
        "        # cov[clase] tiene tamaño nºclases x nºclases con lo cual despues de esta mult nos queda una matriz de tamaño nºdatos x nº clases. multiplicamos esta matriz elemento a elemento por mean_x0[clase] y sumamos por filas.\n",
        "        for clase in range(0,np.shape(mean_x0)[0]):\n",
        "          result[clase] = -0.5*self.ln_determinants[clase] + -0.5*(mean_x0[clase].dot(self.inv_covs[clase])*mean_x0[clase]).sum(1) + self.ln_apriories[clase]\n",
        "        return result\n",
        "    def predict_label_estadistico(self,y,grado_pertenencia):\n",
        "        pred = np.argmax(grado_pertenencia,axis=0)\n",
        "        num_aciertos = (pred == y).sum()\n",
        "        media_aciertos = (pred == y).mean()*100\n",
        "        print(f\"Numero de Aciertos es: {num_aciertos}, Porcentaje de aciertos: {media_aciertos}\")\n",
        "    def clasfBay(X,y,test_size=0.3):\n",
        "      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
        "      clasfXBayesiano = ClassifBayesiano()\n",
        "      clasfXBayesiano.fit_estadistico(X_train,y_train)\n",
        "      grado = clasfXBayesiano.predict_estadistico(X_test)\n",
        "      clasfXBayesiano.predict_label_estadistico(y_test,grado)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v15yk3XCfgfG",
        "colab_type": "text"
      },
      "source": [
        "### 3.3 Tabla de resultados. Discusión y comparadlos con el de distancia euclidea"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5fKiGLyIsj1",
        "colab_type": "text"
      },
      "source": [
        "#### 3.3.1 Taba de resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSTHBR1Ln7DI",
        "colab_type": "text"
      },
      "source": [
        "Cogiendo una muestra de test del 50% obtenemos los siguientes resultados para los tres datasets estudiados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0GOrUVgqlgp",
        "colab_type": "code",
        "outputId": "baf9b9ce-d065-4074-ea6a-69e27236bbf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "X, y  = load_iris(return_X_y=True)\n",
        "print(\"Datos Iris Bayesiano:\")\n",
        "ClassifBayesiano.clasfBay(X,y,0.5)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Datos Iris Bayesiano:\n",
            "Numero de Aciertos es: 71, Porcentaje de aciertos: 94.66666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sStHIEt6op5L",
        "colab_type": "code",
        "outputId": "94d58e16-2f4d-4bde-9f6c-b8270fa8cb28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "X, y  = load_wine(return_X_y=True)\n",
        "print(\"Datos Wine Bayesiano:\")\n",
        "ClassifBayesiano.clasfBay(X,y,0.5)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Datos Wine Bayesiano:\n",
            "Numero de Aciertos es: 84, Porcentaje de aciertos: 94.3820224719101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUzsfYqHowJ7",
        "colab_type": "code",
        "outputId": "edcbc52b-760c-42ea-9f56-9714f8144794",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "X, y  = load_breast_cancer(return_X_y=True)\n",
        "print(\"Datos Cancer Bayesiano:\")\n",
        "ClassifBayesiano.clasfBay(X,y,0.5)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Datos Cancer Bayesiano:\n",
            "Numero de Aciertos es: 268, Porcentaje de aciertos: 94.03508771929825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHFdTHL7IzkC",
        "colab_type": "text"
      },
      "source": [
        "Medida |Iris | Wine | Cancer\n",
        "--- |--- | --- | ---\n",
        "Numero Aciertos|72 | 84 | 272\n",
        "Porcentaje Aciertos|96% | 94% | 95%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjwUTLsDJPSW",
        "colab_type": "text"
      },
      "source": [
        "#### 3.3.2 Discusión"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaSzpg1cJS-J",
        "colab_type": "text"
      },
      "source": [
        "Se puede observar que el porcentaje de aciertos es del 95% aproximadamente para las 3 bases de datos. A diferencia que en el clasificador de la distancia euclídea, este clasificador funciona fantásticamente para las 3 bases de datos y no para 2. Esto es debido a que este clasificador se basa en la estadística y no en la dispersión de las clases. No obstante, como se observará más adelante, todavía se puede mejorar ligeramente este clasificador añadiendole dos parámetros para ajustar más finamente los resultados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvwKgN1_plN3",
        "colab_type": "text"
      },
      "source": [
        "## 4. Regularización\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGSXd5vrf50I",
        "colab_type": "text"
      },
      "source": [
        "### 4.1 Explica brevemente en qué consiste la regularización y justifica su necesidad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyE3t_eZOVTO",
        "colab_type": "text"
      },
      "source": [
        "La regularización consiste en utilizar dos parámetros que determinan el comportamiento de nuestro clasificador. El primero será una variable booleana que determinará si las clases comparten matriz de covarianza o no y otra variable será un número entre 0-1 llamado shrinkage que consiste en reducir el ratio entre el menor autovalor y el mayor de la matriz de covarianza. Por lo tanto, nuestro clasificador bayesiano paramétrico será igual que el bayesiano calculado arriba pero teniendo en cuenta estas dos variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv88wnwngApo",
        "colab_type": "text"
      },
      "source": [
        "### 4.2 Implementa el clasificador Estadístico Bayesiano Paramétrico"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVMXbKO2OV0q",
        "colab_type": "text"
      },
      "source": [
        "A continuación se muestra la clase de nuestro clasificador bayesiano paramétrico"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaahEWJUDNGH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.covariance import ShrunkCovariance\n",
        "from sklearn import preprocessing\n",
        "from abc import abstractmethod\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "\n",
        "class Classifier(BaseEstimator):\n",
        "\n",
        "    @abstractmethod\n",
        "    def fit(self,X,y):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self,X):\n",
        "        pass\n",
        "\n",
        "class ClassifBayesianoParametrico(Classifier):\n",
        "    def __init__(self, share_covs=False, shrinkage=0.0):\n",
        "        \"\"\"Constructor de la clase\n",
        "        share_covs: Indica si la matriz de covarianzas va a ser compartida entre las distintas clases.\n",
        "        shrinkage: Parámetro que determina la diagonalidad de la matriz de covarianzas. Ver sklearn.covariance.ShrunkCovariance\n",
        "        \"\"\"\n",
        "        assert 0 <= shrinkage <= 1\n",
        "        self.labels = None\n",
        "        self.ln_apriories = None\n",
        "        self.means = None\n",
        "        self.ln_determinants = None\n",
        "        self.inv_covs = None\n",
        "        self.share_covs = share_covs\n",
        "        self.shrinkage = shrinkage\n",
        "        self.scaler = preprocessing.StandardScaler()\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Entrena el clasificador\n",
        "        X: matriz numpy cada fila es un dato, cada columna una medida\n",
        "        y: vector de etiquetas, tantos elementos como filas en X\n",
        "        retorna objeto clasificador\"\"\"\n",
        "        assert X.ndim == 2 and X.shape[0] == len(y)\n",
        "        # Preprocesamos los datos de entrada\n",
        "        self.scaler.fit(X)   \n",
        "        X = self.scaler.transform(X)\n",
        "        # Contar cuantos ejemplos hay de cada etiqueta\n",
        "        unique, counts = np.unique(y,return_counts=True)\n",
        "        self.labels = unique\n",
        "        # Usando el contador de ejemplos de cada etiqueta, calcular el logaritmo neperiano de las probabilidades a-priori\n",
        "        prob_priori = np.divide(counts,y.sum())\n",
        "        self.ln_apriories = np.log(prob_priori)\n",
        "        # Calcular para los ejemplos de cada clase, la media de cada una de sus características (centroide)\n",
        "        self.means = np.array([np.mean(X[y==l],axis=0) for l in np.unique(y)])\n",
        "        #resta = np.array([X[y==l]-self.means[l] for l in np.unique(y)])\n",
        "        resta = [X[y==l] - self.means[l] for l in self.labels]\n",
        "        if self.share_covs:\n",
        "            # restar a cada dato la media de su clase y llamar una sola vez a ShrunkCovariance(self.shrinkage).fit(datos_menos_la_media_de_cada_clase). \n",
        "            # Restamos al dato de cada clase su centroide\n",
        "            # Calcula la matriz de covarianzas empleando la clase ShrunkCovariance de sklearn\n",
        "            resta = np.vstack(resta)\n",
        "            resta = resta.reshape(-1, X.shape[1])\n",
        "            cov1 = np.array(ShrunkCovariance(self.shrinkage).fit(resta).covariance_ )\n",
        "            # La reproducimos tantas veces como número de clases\n",
        "            cov = np.empty((len(unique),X.shape[1],X.shape[1]))\n",
        "            for l in range(0,len(unique)):\n",
        "              cov[l] = cov1\n",
        "        else:\n",
        "            # Calcula la matriz de covarianzas empleando la clase ShrunkCovariance de sklearn\n",
        "            cov = [ShrunkCovariance(self.shrinkage).fit(resta[l]).covariance_ for l in unique]\n",
        "            \n",
        "\n",
        "        # Para cada una de las clases, calcular el logaritmo neperiano de su matriz de covarianzas (puedes emplear compresión de listas o la función map)\n",
        "        self.ln_determinants = np.log(np.array(np.linalg.det(cov)))\n",
        "        # Para cada una de las clases, calcular la inversa de su matriz de covarianzas (puedes emplear compresión de listas o la función map)\n",
        "        self.inv_covs = np.linalg.inv(cov)\n",
        "        \n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Estima el grado de pertenencia de cada dato a todas las clases\n",
        "        X: matriz numpy cada fila es un dato, cada columna una medida del vector de caracteristicas.\n",
        "        Retorna una matriz, con tantas filas como datos y tantas columnas como clases tenga\n",
        "        el problema, cada fila almacena los valores pertenencia de un dato a cada clase\"\"\"\n",
        "        assert self.means is not None, \"Error: The classifier needs to be fitted. Please call fit(X, y) method.\"\n",
        "        assert X.ndim == 2 and X.shape[1] == self.means.shape[1]\n",
        "\n",
        "        # Preprocesamos nuestros datos\n",
        "        X = self.scaler.transform(X)\n",
        "        mean_x0 = np.array(X - self.means[:,np.newaxis])\n",
        "        result = np.empty(((len(self.labels),np.shape(X)[0])))\n",
        "        for clase in range(0,np.shape(mean_x0)[0]):\n",
        "          result[clase] = -0.5*self.ln_determinants[clase] + -0.5*(mean_x0[clase].dot(self.inv_covs[clase])*mean_x0[clase]).sum(1) + self.ln_apriories[clase]\n",
        "        return result\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Estima la etiqueta de cada dato. La etiqueta puede ser un entero o bien un string.\n",
        "        X: matriz numpy cada fila es un dato, cada columna una medida\n",
        "        retorna un vector con las etiquetas de cada dato\"\"\"\n",
        "        grado_pertenencia = self.predict_proba(X)\n",
        "        pred = np.argmax(grado_pertenencia,axis=0)\n",
        "        return pred\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHH3pZny7FyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def best_shrinkage_clf(X, y, k, shrinkages, share_covs):\n",
        "    \"\"\"\n",
        "    Busca el clasificador bayesiano regularizado con el mejor shrinkage. \n",
        "    :param X: Ejemplos de la dase de datos\n",
        "    :param y: Etiquetas de los ejemplos\n",
        "    :param k: Número de divisiones en la validación cruzada (k-fold)\n",
        "    :param shrinkages: Lista de posibles shrinkages que conforman la rejilla de búsqueda\n",
        "    :param share_covs: Lista de posibles valores para share_covs que conforman la rejilla de búsqueda\n",
        "    \"\"\"\n",
        "    from sklearn.model_selection import GridSearchCV\n",
        "    cbp = ClassifBayesianoParametrico(share_covs)\n",
        "    params = {'shrinkage': shrinkages, 'share_covs': share_covs}\n",
        "    clf = GridSearchCV(cbp, params, n_jobs=-2, scoring='accuracy', cv=k).fit(X, y)\n",
        "    best_clf = clf.best_estimator_\n",
        "    # print(\"Srinkage scores: \", clf.cv_results_['mean_test_score'])\n",
        "    result_score_mean = clf.cv_results_['mean_test_score'][clf.best_index_]\n",
        "    result_score_std = clf.cv_results_['std_test_score'][clf.best_index_]\n",
        "    print(\"\\tSelected shrinkage = {}, share_covs = {}\\n\" \\\n",
        "          \"\\tAccuracy: {:.3f} (+/- {:.3f})\".format(best_clf.shrinkage,\n",
        "                                                   best_clf.share_covs,\n",
        "                                                   result_score_mean,\n",
        "                                                   result_score_std))\n",
        "    return best_clf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp-x8RvvJ6Zh",
        "colab_type": "text"
      },
      "source": [
        "## 5. Evaluación del Rendimiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzFeDOhlplN9",
        "colab_type": "text"
      },
      "source": [
        "Aplicando el código proporcionado por la asignatura a los datos del iris obtenemos los siguientes datos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJhoDHs87AXb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "c643aa0e-bccf-4e8c-f06b-dce403c36527"
      },
      "source": [
        "import sklearn\n",
        "from time import time\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import confusion_matrix\n",
        "dataset = load_iris()\n",
        "X = dataset.data\n",
        "y = dataset.target\n",
        "# 2. Baraja los datos para hacer validación cruzada\n",
        "X, y = sklearn.utils.shuffle(X, y)\n",
        "# Evaluar el clasificador de la distancia eucídea usando cross validation (k-fold=5)\n",
        "k = 5\n",
        "clf = ClassifEuclid()\n",
        "print(\"Clasificador Euclídeo\")\n",
        "scores = cross_val_score(clf,X,y,cv=5)\n",
        "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
        "\n",
        "# Haz una selección de modelos para buscar shrinkage del clasificador \n",
        "# estadístico paramétrico que obtiene el mejor accuracy. Usa para ello cross validation (k-fold=5)\n",
        "print(\"Clasificador Estadístico Paramétrico\")\n",
        "clf = best_shrinkage_clf(X, y, k, [1,0.7,0.15,0.1,0.2,0.3,0.5,0.4], [True,False])\n",
        "\n",
        "# Predice las muestras de la base de datos y muestra la matriz de confusión (sklearn.metrics.confusion_matrix)\n",
        "clf.fit(X,y)\n",
        "print(f\"Matriz de Confusión:\\n {confusion_matrix(y, clf.predict(X))}\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clasificador Euclídeo\n",
            "Accuracy: 95.33 (+/- 5.33)\n",
            "Clasificador Estadístico Paramétrico\n",
            "\tSelected shrinkage = 1, share_covs = True\n",
            "\tAccuracy: 0.973 (+/- 0.025)\n",
            "Matriz de Confusión:\n",
            " [[50  0  0]\n",
            " [ 0 47  3]\n",
            " [ 0  1 49]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvYxY75o7-_n",
        "colab_type": "text"
      },
      "source": [
        "Para los datos de wine:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P7LmTi37-K-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "ca554841-53a0-4d9e-bc0d-123f26f6c2c7"
      },
      "source": [
        "# 1. Cargar los datos de la base de datos de entrenamiento\n",
        "from sklearn.datasets import load_wine\n",
        "dataset = load_wine()\n",
        "X = dataset.data\n",
        "y = pd.factorize(dataset.target)[0]\n",
        "# 2. Baraja los datos para hacer validación cruzada\n",
        "X, y = sklearn.utils.shuffle(X, y)\n",
        "\n",
        "# Evaluar el clasificador de la distancia eucídea usando cross validation (k-fold=5)\n",
        "k = 5\n",
        "print(\"Clasificador Euclídeo\")\n",
        "clf = ClassifEuclid()\n",
        "scores = cross_val_score(clf,X,y,cv=5)\n",
        "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
        "\n",
        "print(\"Clasificador Estadístico Paramétrico\")\n",
        "\n",
        "clf = best_shrinkage_clf(X, y, k, [0.92,0.2,0.3,0.5,0.4,0.7], [True,False])\n",
        "\n",
        "# Predice las muestras de la base de datos y muestra la matriz de confusión (sklearn.metrics.confusion_matrix)\n",
        "clf.fit(X,y)\n",
        "print(\"\")\n",
        "confusion_matrix(y, clf.predict(X))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clasificador Euclídeo\n",
            "Accuracy: 71.92 (+/- 24.06)\n",
            "Clasificador Estadístico Paramétrico\n",
            "\tSelected shrinkage = 0.92, share_covs = True\n",
            "\tAccuracy: 1.000 (+/- 0.000)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[59,  0,  0],\n",
              "       [ 0, 71,  0],\n",
              "       [ 0,  0, 48]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTJcNOOI8EaL",
        "colab_type": "text"
      },
      "source": [
        "Finalmente para los datos del cancer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPRaasZO8HY1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "10b50582-e6a2-4621-cdd8-86506d9dfbc8"
      },
      "source": [
        "# 1. Cargar los datos de la base de datos de entrenamiento\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "dataset = load_breast_cancer()\n",
        "X = dataset.data\n",
        "y = pd.factorize(dataset.target)[0]\n",
        "X, y = sklearn.utils.shuffle(X, y)\n",
        "\n",
        "# Evaluar el clasificador de la distancia eucídea usando cross validation (k-fold=5)\n",
        "k = 5\n",
        "print(\"Clasificador Euclídeo\")\n",
        "clf = ClassifEuclid()\n",
        "scores = cross_val_score(clf,X,y,cv=5)\n",
        "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
        "\n",
        "print(\"Clasificador Estadístico Paramétrico\")\n",
        "clf = best_shrinkage_clf(X, y, k, [0.71,0.3,0.5,0.4,0.7], [True,False])\n",
        "\n",
        "# Predice las muestras de la base de datos y muestra la matriz de confusión (sklearn.metrics.confusion_matrix)\n",
        "clf.fit(X,y)\n",
        "print(\"\")\n",
        "confusion_matrix(y, clf.predict(X))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clasificador Euclídeo\n",
            "Accuracy: 89.80 (+/- 6.27)\n",
            "Clasificador Estadístico Paramétrico\n",
            "\tSelected shrinkage = 0.71, share_covs = True\n",
            "\tAccuracy: 0.961 (+/- 0.016)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[192,  20],\n",
              "       [  1, 356]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJKSLDtB8UAC",
        "colab_type": "text"
      },
      "source": [
        "Analizando los resultados obtenidos podemos ver como el clasificador estadístico paramétrico arroja mejores resultados para los datasets del iris y wine pero peores para el del cancer. Cabe mencionar el 100% de acierto del clasificador paramétrico en los datos del wine mientras que el euclídeo solo es capaz de acertar un 72% de las veces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gR9JxoBAwXV",
        "colab_type": "text"
      },
      "source": [
        "## 6. Aplicación en un caso real de reconocimiento de texto\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmMn9SajplN_",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Para esta aplicación utilizaremos el clasificador euclídeo. Utilizaremos este clasificador porque creo que las dispersión de las clases es pequeña en relación a la distancia entre ella, o al menos en la mayoría de los casos. Para empezar, eliminaremos ciertos carácteres que no queramos que nuestro clasificador detecte, esto, obviamente, solo es válido para el texto de prueba, pero puede ser un buen primer paso para, más adelante, depurarlo un poco más. Eliminaremos los carácteres 'w','x','k' y 'z' así como los números '5','7' y '8'. A parte de quitar estoss carácteres, añadiremos algunos otros como á y ú y - y :. Más adelante veremos que los carácteres que detecte como á y ú los mapearemos a las letras a y u. También, para nuestros datos de entrenamiento, utilizaremos ejemplos sin ruido y ejemplos con ruido en una relación 1:2 aproximadamente siendo los ejemplos con ruido los más numerosos. Con solo estos cambios ya somos capaces de obtener un porcentaje de aciertos del 80%.\n",
        "\n",
        "La segunda tanda de cambios que realizaremos tendrán que ver en como procesamos los dígitos detectados. Lo primero que haremos será aumentar los píxeles (a por ejemplo 3000 pixeles) de la foto para que se vea con más claridad y estén más claros los símbolos. Agregaremos un filtro que, a partir de un threshold, considerará pixeles que sean medio oscuros o medio claros como oscuros y claros. Modificaremos el split a 1 (antes 1.5) y cambiaremos un poco el bitmap para que sea más alargado (28x25) con todos estos cambios obtenemos un porcentaje de aciertos del 93.4% (de media), para casos de prueba que no son el proporcionado por el profesor. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kifbWXuAzIn",
        "colab_type": "text"
      },
      "source": [
        "## 7. Conclusiones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN18c2FEplOF",
        "colab_type": "text"
      },
      "source": [
        "A lo largo de esta práctica hemos visto como desarrollar distintos clasificadores y como evaluar el rendimiento de los mismos. Finalmente, hemos aplicado los conocimientos adquiridos a un caso real. Esta práctica muestra como clasificadores muy sencillos son capaces de resolver problemas de clasificacion con cierta facilidad llegando incluso a porcentajes de acierto superiores al 90%."
      ]
    }
  ]
}